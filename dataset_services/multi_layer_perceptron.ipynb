{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dslabs_functions as dslabs\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "import dslabs_functions as dslabs\n",
    "\n",
    "file_tag = \"Credit_Score\"\n",
    "## Train - resultado do balancing escolhido (over no nosso caso)\n",
    "train_filename = \"data_preparation_final_results_1/data_preparation_csvs/balancing_csvs/data_balancing_alt2_over.csv\"\n",
    "## ficheiro Teste ao qual nao fazemos balancing\n",
    "test_filename = \"data_preparation_final_results_1/data_preparation_csvs/feat_eng_csvs/test_data_feat_select_alt3_RFE.csv\"\n",
    "target = \"Credit_Score\"\n",
    "# default\n",
    "eval_metric = \"recall\"\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = dslabs.read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric=\"recall\"\n",
    "figure()\n",
    "best_model, params = dslabs.mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=1000,\n",
    "    lag=250,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "print(best_model)\n",
    "print(params)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric=\"precision\"\n",
    "figure()\n",
    "best_model, params = dslabs.mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=1000,\n",
    "    lag=250,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "print(best_model)\n",
    "print(params)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric=\"accuracy\"\n",
    "figure()\n",
    "best_model, params = dslabs.mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=1000,\n",
    "    lag=250,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "print(best_model)\n",
    "print(params)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric=\"auc\"\n",
    "figure()\n",
    "best_model, params = dslabs.mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=1000,\n",
    "    lag=250,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "print(best_model)\n",
    "print(params)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric=\"f1\"\n",
    "figure()\n",
    "best_model, params = dslabs.mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=1000,\n",
    "    lag=250,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "print(best_model)\n",
    "print(params)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: np.array = best_model.predict(trnX)\n",
    "prd_tst: np.array = best_model.predict(tstX)\n",
    "figure()\n",
    "dslabs.plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'classification_images/mlp/{file_tag}_mlp_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "eval_metric=\"recall\"\n",
    "lr_type: Literal[\"constant\", \"invscaling\", \"adaptive\"] = params[\"params\"][0]\n",
    "lr: float = params[\"params\"][1]\n",
    "nr_iterations: list[int] = [i for i in range(100, 1001, 100)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric = \"recall\"\n",
    "\n",
    "warm_start: bool = False\n",
    "for n in nr_iterations:\n",
    "    clf = MLPClassifier(\n",
    "        warm_start=warm_start,\n",
    "        learning_rate=lr_type,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=n,\n",
    "        activation=\"logistic\",\n",
    "        solver=\"sgd\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: np.array = clf.predict(tstX)\n",
    "    prd_trn_Y: np.array = clf.predict(trnX)\n",
    "    y_tst_values.append(dslabs.CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(dslabs.CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "    warm_start = True\n",
    "\n",
    "figure()\n",
    "dslabs.plot_multiline_chart(\n",
    "    nr_iterations,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"MLP overfitting study for lr_type={lr_type} and lr={lr}\",\n",
    "    xlabel=\"nr_iterations\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"classification_images/mlp/{file_tag}_mlp_{eval_metric}_overfitting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "eval_metric=\"precision\"\n",
    "lr_type: Literal[\"constant\", \"invscaling\", \"adaptive\"] = params[\"params\"][0]\n",
    "lr: float = params[\"params\"][1]\n",
    "nr_iterations: list[int] = [i for i in range(100, 1001, 100)]\n",
    "\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric = \"precision\"\n",
    "\n",
    "warm_start: bool = False\n",
    "for n in nr_iterations:\n",
    "    clf = MLPClassifier(\n",
    "        warm_start=warm_start,\n",
    "        learning_rate=lr_type,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=n,\n",
    "        activation=\"logistic\",\n",
    "        solver=\"sgd\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: np.array = clf.predict(tstX)\n",
    "    prd_trn_Y: np.array = clf.predict(trnX)\n",
    "    y_tst_values.append(dslabs.CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(dslabs.CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "    warm_start = True\n",
    "\n",
    "figure()\n",
    "dslabs.plot_multiline_chart(\n",
    "    nr_iterations,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"MLP overfitting study for lr_type={lr_type} and lr={lr}\",\n",
    "    xlabel=\"nr_iterations\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"classification_images/mlp/{file_tag}_mlp_{eval_metric}_overfitting.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
